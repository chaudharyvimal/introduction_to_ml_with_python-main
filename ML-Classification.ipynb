{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5fda092",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92502854",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import chi2_contingency\n",
    "import mglearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec872a7",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78dbfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "cancer = load_breast_cancer()\n",
    "dfraw = pd.DataFrame(cancer['data'], columns=cancer['feature_names'])\n",
    "dfraw = dfraw.loc[:, 'mean radius':'mean perimeter']  # use only first 3 features for simplicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149cf87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a categorical 'state' column (example: random assignment from a small set)\n",
    "states = ['NY', 'CA', 'TX', 'FL', 'IL']\n",
    "np.random.seed(0)\n",
    "dfraw['state'] = pd.Categorical(np.random.choice(states, size=len(dfraw)))\n",
    "\n",
    "product = ['a', 'b', 'c']\n",
    "dfraw['product'] = pd.Categorical(np.random.choice(product, size=len(dfraw)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0f8708",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfraw['target'] = cancer['target']\n",
    "# convert numeric target to categorical labels\n",
    "dfraw['target'] = pd.Categorical(dfraw['target'])\n",
    "\n",
    "dfraw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0e242b",
   "metadata": {},
   "source": [
    "# Data Partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2410b36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dftemp=dfraw.drop(columns='target',inplace=False)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    dftemp, dfraw['target'], stratify=cancer.target, random_state=42)\n",
    "\n",
    "dftrain=pd.concat([X_train, y_train], axis=1)\n",
    "dftrain['traintest']= 'train'\n",
    "\n",
    "dftest=pd.concat([X_test, y_test], axis=1)\n",
    "dftest['traintest']= 'test'\n",
    "\n",
    "dfall = pd.concat([dftrain, dftest], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae16301",
   "metadata": {},
   "source": [
    "# EDA (Exploratory Data Analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80082a4f",
   "metadata": {},
   "source": [
    "## Overall Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04868abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build dataframe summary with missing counts & percentages\n",
    "dfsummary = pd.DataFrame({\n",
    "    'Variable': dftrain.columns,\n",
    "    'DataType': dftrain.dtypes.astype(str),\n",
    "    'total_count': len(dftrain),\n",
    "    'non_null_count': dftrain.notnull().sum().values,\n",
    "    'missing_count': dftrain.isnull().sum().values,\n",
    "    'missing_pct': (dftrain.isnull().mean() * 100).round(2).values,\n",
    "    'n_unique': dftrain.nunique().values\n",
    "})\n",
    "\n",
    "# optional: include up to 3 sample values per column\n",
    "dfsummary['samples'] = dftrain.apply(lambda s: s.dropna().unique()[:3].tolist()).values\n",
    "\n",
    "dfsummary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous-Continuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41c4917",
   "metadata": {},
   "outputs": [],
   "source": [
    "dftrain.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acae92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.PairGrid(dftrain, hue='target',diag_sharey=False)\n",
    "\n",
    "def corrfunc(x, y, **kws):\n",
    "    # r = np.corrcoef(x, y)[0][1]\n",
    "    # dftemp=pd.DataFrame(data={'x':x, 'y':y})\n",
    "    # r = dftemp.corr().round(3).iloc[0,1]\n",
    "    \n",
    "    r = dfraw[[x.name, y.name]].corr().iloc[0, 1].round(3)\n",
    "\n",
    "    ax = plt.gca()\n",
    "    # ax.clear()\n",
    "    # ax.spines['left'].set_visible(False)\n",
    "    # ax.spines['bottom'].set_visible(False)\n",
    "    # ax.set_xticks([])\n",
    "    # ax.set_yticks([])\n",
    "    ax.text(.5,.5,f\"$\\\\rho = {r}$\",transform=ax.transAxes)\n",
    "\n",
    "g.map_upper(corrfunc) \n",
    "g.map_diag(sns.kdeplot, fill=True)\n",
    "g.map_lower(sns.scatterplot)\n",
    "\n",
    "# Add title\n",
    "g.fig.suptitle('Pairplot with Correlation in Upper Triangle', fontsize=16, y=1.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb75294",
   "metadata": {},
   "source": [
    "## Categorical-Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98efb6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcat = dftrain.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "#dfcat = dfsummary[dfsummary['DataType'].isin(['category'])]\n",
    "\n",
    "fig, axs = plt.subplots(nrows=len(dfcat),ncols = len(dfcat), figsize=(5*len(dfcat),4*len(dfcat)),sharey=True)\n",
    "for i, col in enumerate(dfcat):\n",
    "    for j in range(len(dfcat)):\n",
    "        if i==j:\n",
    "            axs[i,j].hist(data=dftrain[dfcat], x=col)\n",
    "            # sns.histplot(data=df, x=col, multiple='stack', stat='proportion')\n",
    "            axs[i,j].set_xlabel(col)\n",
    "            axs[i,j].set_ylabel('Count')\n",
    "        elif i>j:\n",
    "            # axs[i,j].hist(data=df[dfcat], x=col, stacked=True)\n",
    "            sns.histplot(\n",
    "                data=dftrain,\n",
    "                x=col,\n",
    "                hue=dfcat[j],            # stack by this categorical column\n",
    "                multiple=\"stack\",\n",
    "                stat=\"count\",\n",
    "                shrink=0.8,\n",
    "                ax=axs[i, j]\n",
    "            )\n",
    "            axs[i,j].set_xlabel(col)\n",
    "            axs[i,j].set_ylabel('Count')\n",
    "        elif i<j:\n",
    "            contingency_table = pd.crosstab(dfraw[col], dfraw[dfcat[j]])\n",
    "            chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "            axs[i,j].text(.4,.4,f\"$\\chi^2 = {chi2:.2f}$\" + \"\\n\" f\"p-value = {p:.2f}\" + \"\\n\" f\"df = {dof:.2f}\",\n",
    "            transform=axs[i,j].transAxes)\n",
    "\n",
    "\n",
    "fig.suptitle('Categorical Variables Distribution', fontsize=16)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous-Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e92b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "dfnum = dftrain.select_dtypes(include=['number']).columns.tolist()\n",
    "# cat_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "if len(dfnum) == 0 or len(dfcat) == 0:\n",
    "    print(\"Need at least one numeric and one categorical column.\")\n",
    "else:\n",
    "    for cat in dfcat:\n",
    "        n = len(dfnum)\n",
    "        ncols = 3\n",
    "        nrows = math.ceil(n / ncols)\n",
    "        fig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=(5 * ncols, 4 * nrows), squeeze=False)\n",
    "        axs_flat = axs.flatten()\n",
    "\n",
    "        for i, num in enumerate(dfnum):\n",
    "            ax = axs_flat[i]\n",
    "            if 'target' in dfraw.columns:\n",
    "                sns.boxplot(data=dfraw, x=cat, y=num, hue='target', ax=ax)\n",
    "                ax.legend(loc='upper right', fontsize='small')\n",
    "            else:\n",
    "                sns.boxplot(data=dfraw, x=cat, y=num, ax=ax)\n",
    "            ax.set_title(f\"{num} by {cat}\")\n",
    "            ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "        # turn off any unused subplots\n",
    "        for j in range(n, len(axs_flat)):\n",
    "            axs_flat[j].axis('off')\n",
    "\n",
    "        fig.suptitle(f'Boxplots of numeric variables grouped by \"{cat}\"', fontsize=14, y=1.05)\n",
    "        plt.tight_layout()\n",
    "        fig.subplots_adjust(top=0.92)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fc0cc0",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8d807e",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c940399d",
   "metadata": {},
   "source": [
    "### Binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076a85d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KBinsDiscretizer\n",
    "bins = np.linspace(43, 190, 5)\n",
    "print(\"bins: {}\".format(bins))\n",
    "\n",
    "dfall['mean perimeter_binned'] = pd.cut(dfall['mean perimeter'], bins=bins, include_lowest=True)\n",
    "dfall\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09afb81",
   "metadata": {},
   "source": [
    "### Dummy Variables, One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54ade07",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfall_dummy = pd.get_dummies(dfall, columns=['mean perimeter_binned','state', 'product','target'], drop_first=True)\n",
    "dfall_dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c5a4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "cat_variables = ['mean perimeter_binned','state', 'product', 'target']\n",
    "\n",
    "enc = OneHotEncoder(drop='first')\n",
    "enc.fit(dfall.loc[dfall['traintest'] == 'train',cat_variables])\n",
    "\n",
    "df_encoded = enc.transform(dfall[cat_variables])\n",
    "df_encoded = pd.DataFrame(df_encoded.toarray(), columns=enc.get_feature_names_out(cat_variables))\n",
    "dfall_encoded = dfall.drop(columns=cat_variables).reset_index(drop=True).join(df_encoded)\n",
    "dfall_encoded\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28967476",
   "metadata": {},
   "source": [
    "### Interactions and Polynomials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61607b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "num_variables = ['mean perimeter', 'mean texture']\n",
    "\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False,interaction_only=True)\n",
    "poly.fit(dfall.loc[dfall['traintest'] == 'train',num_variables])\n",
    "\n",
    "df_poly = poly.transform(dfall[num_variables])\n",
    "df_poly = pd.DataFrame(df_poly, columns=poly.get_feature_names_out(num_variables))\n",
    "dfall_poly = dfall.drop(columns=num_variables).reset_index(drop=True).join(df_poly)\n",
    "dfall_poly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bffe2a8",
   "metadata": {},
   "source": [
    "### Univariate non-linear transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f5a6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp, log\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2311bb",
   "metadata": {},
   "source": [
    "## Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40378ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# StandardScaler - z = (x - u) / s (transformed data has µ=0, σ =1)\n",
    "# RobustScaler   - x_trans = (x - x_median) / IQR\n",
    "# MinMaxScaler   - x_trans = (x - min) / (max-min)\n",
    "# Normalizer     - rescales so Euclidean length =1, useful when angle (direction) is imp and not length (magnitude)?\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scale_variables = ['mean perimeter', 'mean texture']\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(dfall.loc[dfall['traintest'] == 'train',num_variables])\n",
    "\n",
    "df_scale = scaler.transform(dfall[num_variables])\n",
    "df_scale = pd.DataFrame(df_scale, columns=scaler.get_feature_names_out(num_variables))\n",
    "dfall_scale = dfall.drop(columns=num_variables).reset_index(drop=True).join(df_scale)\n",
    "\n",
    "dfall_scale[scale_variables].describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa8a13c",
   "metadata": {},
   "source": [
    "## Missing Value Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caaf84ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns with >5% missing values\n",
    "cols_to_drop = dfsummary.loc[dfsummary['missing_pct']>.05,\"Variable\"].to_list()\n",
    "dftrainmv = dftrain.drop(columns=cols_to_drop)\n",
    "dftrainmv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cac9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute columns wuth <5% missing values\n",
    "\n",
    "# num columns\n",
    "dfnum = dftrainmv.select_dtypes(include=['number']).columns.tolist()\n",
    "for i, var in enumerate(dfnum):\n",
    "    dftrainmv[var].fillna(dftrainmv[var].mean(), inplace=True) \n",
    "\n",
    "# cat columns\n",
    "dfcat = dftrainmv.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "for i, var in enumerate(dfnum):\n",
    "    dftrainmv[var].fillna(dftrainmv[var].mode()[0], inplace=True)\n",
    "\n",
    "dftrainmv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8110c7ff",
   "metadata": {},
   "source": [
    "## Outlier Treatment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85370b78",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b53acbf",
   "metadata": {},
   "source": [
    "## Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418d9b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfnum = dftrain.select_dtypes(include=['number'])\n",
    "dfcorr = dfnum.corr().round(3)\n",
    "dfcorr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40191794",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfhicorr = dfcorr[(dfcorr.abs() > 0.7) & (dfcorr.abs() < 1)]\n",
    "dfhicorr = dfhicorr.dropna(how='all').dropna(axis=1, how='all')\n",
    "dfhicorr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410ced68",
   "metadata": {},
   "source": [
    "## VIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6461c6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VIF Calculation using sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "def calculate_vif_sklearn(df):\n",
    "    # Select numeric columns\n",
    "    dfnum = df.select_dtypes(include=['number'])\n",
    "    \n",
    "    vif_data = []\n",
    "    for i in range(dfnum.shape[1]):\n",
    "        y = dfnum.iloc[:, i]\n",
    "        X = dfnum.drop(dfnum.columns[i], axis=1)\n",
    "        \n",
    "        # Fit linear regression\n",
    "        model = LinearRegression()\n",
    "        model.fit(X, y)\n",
    "        \n",
    "        # Compute R²\n",
    "        r2 = model.score(X, y)\n",
    "        \n",
    "        # Compute VIF\n",
    "        vif = 1 / (1 - r2) if r2 < 1 else float('inf')\n",
    "        vif_data.append((dfnum.columns[i], vif))\n",
    "    \n",
    "    return pd.DataFrame(vif_data, columns=['feature', 'VIF'])\n",
    "\n",
    "dftrainvif = calculate_vif_sklearn(dftrainmv)\n",
    "dftrainvif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52fc979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns with high VIF\n",
    "def drop_high_vif(df, threshold=10):\n",
    "    # dftemp=df.copy()\n",
    "\n",
    "    \n",
    "    while True:\n",
    "        dfnum = df.select_dtypes(include=['number'])\n",
    "        dfvif = calculate_vif_sklearn(dfnum)\n",
    "        max_vif = dfvif['VIF'].max()\n",
    "        if max_vif <= threshold:\n",
    "            break\n",
    "        drop_col = dfvif.loc[dfvif['VIF'].idxmax(), 'feature']\n",
    "        print(f\"Dropping column '{drop_col}' with VIF={max_vif:.2f}\")\n",
    "        df = df.drop(columns=[drop_col])\n",
    "    return df, dfvif\n",
    "\n",
    "dftrainvif, dffinalvif = drop_high_vif(dftrainmv, threshold=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7bd84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dftrainvif"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd79b53e",
   "metadata": {},
   "source": [
    "## IV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed767bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IV Calculation\n",
    "def calculate_iv(df, target, bins=10):\n",
    "    iv_dict = {}\n",
    "    for col in df.columns:\n",
    "        if col == target:\n",
    "            continue\n",
    "        # Handle numeric vs categorical\n",
    "        if pd.api.types.is_numeric_dtype(df[col]):\n",
    "            binned= pd.qcut(df[col], bins, duplicates='drop')\n",
    "            groups = df.groupby(binned,observed=True)\n",
    "        else:\n",
    "            groups = df.groupby(col,observed=True)\n",
    "        \n",
    "        # Calculate WOE and IV\n",
    "        iv = 0\n",
    "        for grp, subset in groups:\n",
    "            good = (subset[target] == 0).sum()\n",
    "            bad = (subset[target] == 1).sum()\n",
    "            total_good = (df[target] == 0).sum()\n",
    "            total_bad = (df[target] == 1).sum()\n",
    "            \n",
    "            if good == 0 or bad == 0:\n",
    "                continue  # Avoid division by zero\n",
    "            \n",
    "            dist_good = good / total_good\n",
    "            dist_bad = bad / total_bad\n",
    "            woe = np.log(dist_good / dist_bad)\n",
    "            iv += (dist_good - dist_bad) * woe\n",
    "        \n",
    "        iv_dict[col] = iv\n",
    "        # df = df.drop(columns=[col+'_bin'], inplace=True, errors='ignore')\n",
    "        \n",
    "    return pd.DataFrame(list(iv_dict.items()), columns=['feature', 'IV']).sort_values(by='IV', ascending=False)\n",
    "\n",
    "dftrainiv = calculate_iv(dftrainvif, target='target')\n",
    "dftrainiv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db9967a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns with IV < threshold\n",
    "def drop_low_iv(df, target, threshold=0.02, bins=10):\n",
    "    iv_df = calculate_iv(df, target, bins)\n",
    "    low_iv_cols = iv_df[iv_df['IV'] < threshold]['feature'].tolist()\n",
    "    \n",
    "    print(f\"Dropping columns with IV < {threshold}: {low_iv_cols}\")\n",
    "    \n",
    "    df.drop(columns=low_iv_cols, inplace=True)\n",
    "    return df, iv_df\n",
    "\n",
    "\n",
    "dftrainiv, iv_table = drop_low_iv(dftrainvif, target='target', threshold=0.02)\n",
    "dftrainiv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2260bfd",
   "metadata": {},
   "source": [
    "## Univariate Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678f9440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Univariate stats e.g. Anova\n",
    "\n",
    "# Tests - f_classif, f_regression\n",
    "# Methods - SelectKBest, SelectPercentile\n",
    "\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "\n",
    "drop_variables = ['target_1','traintest']\n",
    "\n",
    "select = SelectPercentile(percentile=50)\n",
    "select.fit(dfall_encoded.loc[dfall_encoded['traintest']== 'train',~dfall_encoded.columns.isin(drop_variables)],\n",
    " y_train)\n",
    "\n",
    "df_select = select.transform(dfall_encoded.loc[:, ~dfall_encoded.columns.isin(drop_variables)])\n",
    "df_select = pd.DataFrame(df_select, columns=select.get_feature_names_out())\n",
    "dfall_select = pd.concat([df_select, dfall_encoded[['target_1','traintest']]],axis=1)\n",
    "\n",
    "print(select.get_feature_names_out())\n",
    "dfall_select\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855904af",
   "metadata": {},
   "source": [
    "## Model Based Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937117b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unlike univariate selection, all features are considered to capture interactions\n",
    "# 1) Linear Models - Ridge model can be used to feature selection (coef >0)\n",
    "# 2) Tree Models    - feature_importances_\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "drop_variables = ['target_1','traintest']\n",
    "\n",
    "select = SelectFromModel(RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "threshold=\"median\")\n",
    "\n",
    "select.fit(dfall_encoded.loc[dfall_encoded['traintest']== 'train',~dfall_encoded.columns.isin(drop_variables)],\n",
    " dfall_encoded.loc[dfall_encoded['traintest']== 'train','target_1'])\n",
    "\n",
    "df_select = select.transform(dfall_encoded.loc[:, ~dfall_encoded.columns.isin(drop_variables)])\n",
    "df_select = pd.DataFrame(dfall_select, columns=select.get_feature_names_out())\n",
    "dfall_select = pd.concat([df_select, dfall_encoded[['target_1','traintest']]],axis=1)\n",
    "\n",
    "dfall_select\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6792bd",
   "metadata": {},
   "source": [
    "## Recursive feature elimination (RFE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a9375b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "drop_variables = ['target_1','traintest']\n",
    "\n",
    "select = RFE(RandomForestClassifier(n_estimators=100, random_state=42),n_features_to_select=3)\n",
    "select.fit(dfall_encoded.loc[dfall_encoded['traintest']== 'train',~dfall_encoded.columns.isin(drop_variables)],\n",
    " dfall_encoded.loc[dfall_encoded['traintest']== 'train','target_1'])\n",
    "\n",
    "df_select = select.transform(dfall_encoded.loc[:, ~dfall_encoded.columns.isin(drop_variables)])\n",
    "df_select = pd.DataFrame(dfall_select, columns=select.get_feature_names_out())\n",
    "dfall_select = pd.concat([df_select, dfall_encoded[['target_1','traintest']]],axis=1)\n",
    "\n",
    "dfall_select\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310b7220",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748395e4",
   "metadata": {},
   "source": [
    "## kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d35a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = dftrainiv.drop(columns='target').select_dtypes(include=['number'])\n",
    "X_test = X_test[X_train.columns]  # ensure test set has same features as train set\n",
    "y_train = dftrainiv['target']\n",
    "\n",
    "print(\"X_train shape is {}\".format(X_train.shape))\n",
    "print(\"X_test shape is {}\".format(X_test.shape))\n",
    "print(\"y_train shape is {}\".format(y_train.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c748d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "ypred = knn.predict_proba(X_test)\n",
    "print(\"ypred shape is {}\".format(ypred.shape))\n",
    "ypred[0:9,:]\n",
    "\n",
    "print(\"Test set score: {:.2f}\".format(knn.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf305bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(10, 3))\n",
    "\n",
    "for n_neighbors, ax in zip([1, 3, 9], axes):\n",
    "    knn = KNeighborsClassifier(n_neighbors=n_neighbors).fit(X_train, y_train)\n",
    "    \n",
    "    disp = DecisionBoundaryDisplay.from_estimator(\n",
    "        knn,\n",
    "        X_test.iloc[:, 0:2],\n",
    "        response_method=\"predict\",\n",
    "        plot_method=\"pcolormesh\",\n",
    "        xlabel=X_test.columns[0],\n",
    "        ylabel=X_test.columns[1],\n",
    "        shading=\"auto\",\n",
    "        alpha=0.5,\n",
    "        ax=ax\n",
    "    )\n",
    "\n",
    "    sns.scatterplot(\n",
    "        x=X_test.iloc[:, 0],\n",
    "        y=X_test.iloc[:, 1],\n",
    "        hue=y_test,\n",
    "        palette='Set1',\n",
    "        edgecolor='k',\n",
    "        ax=ax\n",
    "    )\n",
    "\n",
    "    ax.set_title(f\"k = {n_neighbors}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666faca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_accuracy = []\n",
    "test_accuracy = []\n",
    "# try different n_neighbors\n",
    "k = range(1, 21)\n",
    "for n_neighbors in k:\n",
    "    # build the model\n",
    "    knn = KNeighborsClassifier(n_neighbors=n_neighbors).fit(X_train, y_train)\n",
    "    # record training set accuracy\n",
    "    training_accuracy.append(knn.score(X_train, y_train))\n",
    "    # record generalization accuracy\n",
    "    test_accuracy.append(knn.score(X_test, y_test))\n",
    "\n",
    "# k=list(k)\n",
    "# reciprocal_values = [1 / x for x in k]\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.lineplot(x=k, y=training_accuracy, marker='o', label='training accuracy')\n",
    "sns.lineplot(x=k, y=test_accuracy, marker='o', label='test accuracy')\n",
    "# sns.relplot(x=reciprocal_values, y=training_accuracy, kind=\"line\", label=\"training accuracy\")\n",
    "# sns.relplot(x=reciprocal_values, y=test_accuracy, kind=\"line\", label=\"test accuracy\")\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('kNN accuracy vs k')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c126186",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4989f0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# X_train_temp = X_train_orig.drop(columns= [\"product\",\"state\"])\n",
    "X_train_temp = dftrain.iloc[:, ~dftrain.columns.isin([\"product\",\"state\",\"target\",\"traintest\"])]\n",
    "# X_test_temp = X_test_orig.drop(columns= [\"product\",\"state\"])\n",
    "X_test_temp = dftest.iloc[:, ~dftrain.columns.isin([\"product\",\"state\",\"target\",\"traintest\"])]\n",
    "\n",
    "logreg = LogisticRegression(max_iter=1000).fit(X_train_temp, y_train)\n",
    "\n",
    "pred_logreg = logreg.predict(X_test_temp)\n",
    "\n",
    "print(\"Training set score: {:.3f}\".format(logreg.score(X_train_temp, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(logreg.score(X_test_temp, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d4d67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Function\n",
    "## shape (n_samples,)\n",
    "## -ve -> Class0, +ve -> Class1\n",
    "## Range is arbitrary\n",
    "logreg.decision_function(X_test_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c224c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision Function contours, imshow\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 5))\n",
    "\n",
    "dfraw_temp = dfraw.iloc[:,0:2].to_numpy() \n",
    "X_train_temp = X_train_orig.iloc[:,0:2].to_numpy()\n",
    "X_test_temp = X_test_orig.iloc[:,0:2].to_numpy()\n",
    "logreg = LogisticRegression().fit(X_train_temp, y_train)\n",
    "\n",
    "mglearn.tools.plot_2d_separator(logreg, X_train_temp, ax=axes[0], alpha=.4,fill=True, cm=mglearn.cm2)\n",
    "scores_image = mglearn.tools.plot_2d_scores(logreg, X_train_temp, ax=axes[1],alpha=.4, cm=mglearn.ReBl)\n",
    "for ax in axes:\n",
    "    # plot training and test points\n",
    "    mglearn.discrete_scatter(X_test_temp[:, 0], X_test_temp[:, 1], y_test,markers='^', ax=ax)\n",
    "    # mglearn.discrete_scatter(X_train_temp[:, 0], X_train_temp[:, 1], y_train,markers='o', ax=ax)\n",
    "    ax.set_xlabel(\"Feature 0\")\n",
    "    ax.set_ylabel(\"Feature 1\")\n",
    "cbar = plt.colorbar(scores_image, ax=axes.tolist())\n",
    "axes[0].legend([\"Test class 0\", \"Test class 1\", \"Train class 0\",\"Train class 1\"], ncol=4, loc=(.1, 1.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77447aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg100 = LogisticRegression(C=100).fit(X_train_temp, y_train)\n",
    "print(\"Training set score: {:.3f}\".format(logreg100.score(X_train_temp, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(logreg100.score(X_test_temp, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9087af4",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1dd94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "linear_svm = LinearSVC().fit(X_train_temp, y_train)\n",
    "print(\"Coefficient shape: \", linear_svm.coef_.shape)\n",
    "print(\"Intercept shape: \", linear_svm.intercept_.shape)\n",
    "\n",
    "print(\"Training set score: {:.3f}\".format(linear_svm.score(X_train_temp, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(linear_svm.score(X_test_temp, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6f5559",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "X, y = make_blobs(random_state=42)\n",
    "\n",
    "fig, ax = plt.subplots( figsize=(5, 3))\n",
    "\n",
    "linear_svm = LinearSVC().fit(X, y)\n",
    "\n",
    "disp = DecisionBoundaryDisplay.from_estimator(\n",
    "    linear_svm,\n",
    "    X,\n",
    "    response_method=\"predict\",\n",
    "    plot_method=\"pcolormesh\",\n",
    "    # xlabel=X.columns[0],\n",
    "    # ylabel=X.columns[1],\n",
    "    shading=\"auto\",\n",
    "    alpha=0.5,\n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "sns.scatterplot(\n",
    "    x=X[:, 0],\n",
    "    y=X[:, 1],\n",
    "    hue=y,\n",
    "    palette='Set1',\n",
    "    edgecolor='k',\n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "ax.set_title(f\"Linear SVM Decision Boundary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2875d5f",
   "metadata": {},
   "source": [
    "## Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff1b4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree = DecisionTreeClassifier(max_depth=4, random_state=0)\n",
    "tree.fit(X_train_temp, y_train)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(tree.score(X_train_temp, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(tree.score(X_test_temp, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c1e38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "export_graphviz(tree, out_file=\"tree.dot\", class_names=[\"malignant\", \"benign\"],\n",
    "    # feature_names=X_train_temp.feature_names, \n",
    "    impurity=False, filled=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d89d896",
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "with open(\"tree.dot\") as f:\n",
    "    dot_graph = f.read()\n",
    "graphviz.Source(dot_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015155df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Feature importances:\\n{}\".format(tree.feature_importances_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42820dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importances_cancer(model):\n",
    "    n_features = X_test_temp.shape[1]\n",
    "    plt.barh(range(n_features), model.feature_importances_, align='center')\n",
    "    plt.yticks(np.arange(n_features)\n",
    "    , X_test_temp.columns\n",
    "    )\n",
    "    plt.xlabel(\"Feature importance\")\n",
    "    plt.ylabel(\"Feature\")\n",
    "plot_feature_importances_cancer(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e650e0",
   "metadata": {},
   "source": [
    "# Model Metrics & Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4381e7",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392b0f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics - Confusion metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion = confusion_matrix(y_test, pred_logreg)\n",
    "print(\"Confusion matrix:\\n{}\".format(confusion))\n",
    "\n",
    "# Precision - To control FP, e.g. Clinical trial\n",
    "# Recall      - To control FN, e.g. Cancer prediction\n",
    "# f-score    - To control both, good for imbalance data\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, pred_logreg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef36dfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics - Callibration for imbalanced classes (Validation set, NOT test set)\n",
    "y_pred_lower_threshold = logreg.predict_proba(X_test_temp) > .25\n",
    "y_pred_lower_threshold = pd.DataFrame(y_pred_lower_threshold, columns=['target_0','target_1']).iloc[:,1].astype('category')\n",
    "\n",
    "confusion = confusion_matrix(y_test, y_pred_lower_threshold)\n",
    "print(\"Confusion matrix:\\n{}\".format(confusion))\n",
    "print(classification_report(y_test, y_pred_lower_threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ffbeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision Recall Curve (top right is better) (Validation set, NOT test set)\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, logreg.predict_proba(X_test_temp)[:, 1])\n",
    "\n",
    "# find threshold closest to zero\n",
    "close_zero = np.argmin(np.abs(thresholds-0.5))\n",
    "plt.plot(precision[close_zero], recall[close_zero], 'o', markersize=10,\n",
    "label=\"threshold zero\", fillstyle=\"none\", c='k', mew=2)\n",
    "plt.plot(precision, recall, label=\"precision recall curve\")\n",
    "\n",
    "# svm\n",
    "precision_linear_svm, recall_linear_svm, thresholds_linear_svm = precision_recall_curve(y_test, \n",
    "linear_svm.decision_function(X_test_temp))\n",
    "\n",
    "close_default_linear_svm = np.argmin(np.abs(thresholds_linear_svm))\n",
    "plt.plot(precision_linear_svm[close_default_linear_svm], recall_linear_svm[close_default_linear_svm], '^', c='k',\n",
    "markersize=10, label=\"threshold 0.5 linear_svm\", fillstyle=\"none\", mew=2)\n",
    "plt.plot(precision_linear_svm, recall_linear_svm, label=\"rf\")\n",
    "\n",
    "plt.xlabel(\"Precision\")\n",
    "plt.ylabel(\"Recall\")\n",
    "\n",
    "from sklearn.metrics import average_precision_score\n",
    "ap_logreg = average_precision_score(y_test, logreg.predict_proba(X_test_temp)[:, 1])\n",
    "ap_linear_svm = average_precision_score(y_test, linear_svm.decision_function(X_test_temp))\n",
    "print(\"Average precision of random forest: {:.3f}\".format(ap_logreg))\n",
    "print(\"Average precision of svc: {:.3f}\".format(ap_linear_svm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b07866b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve top left) (Validation set, NOT test set) (Better for Imbalanced class)\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, logreg.decision_function(X_test_temp))\n",
    "plt.plot(fpr, tpr, label=\"ROC Curve\")\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR (recall)\")\n",
    "# find threshold closest to zero\n",
    "close_zero = np.argmin(np.abs(thresholds))\n",
    "plt.plot(fpr[close_zero], tpr[close_zero], 'o', markersize=10,\n",
    "label=\"threshold zero\", fillstyle=\"none\", c='k', mew=2)\n",
    "plt.legend(loc=4)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "logreg_auc = roc_auc_score(y_test, logreg.predict_proba(X_test_temp)[:, 1])\n",
    "print(\"AUC for Logreg: {:.3f}\".format(logreg_auc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20f260d",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b6ad5e",
   "metadata": {},
   "source": [
    "### Cross Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675d2273",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross Validation NOT USED\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(logreg, dfall_encoded.loc[:,~dfall_encoded.columns.isin(drop_variables)],dfall_encoded['target_1'], cv=5)\n",
    "print(\"Average cross-validation score: {:.2f}\".format(scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d7cb27",
   "metadata": {},
   "source": [
    "### k-fold cross validation, stratified k-fold cross validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6abad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-fold cross validation, stratified k-fold cross validation\n",
    "from sklearn.model_selection import KFold\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "scores = cross_val_score(\n",
    "    logreg, \n",
    "    dfall_encoded.loc[:,~dfall_encoded.columns.isin(drop_variables)],\n",
    "    dfall_encoded['target_1'], \n",
    "    cv=kfold,\n",
    "    scoring = \"accuracy\")\n",
    "print(\"Average cross-validation score: {:.2f}\".format(scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d538b218",
   "metadata": {},
   "source": [
    "### Leave-one-out cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1502ce2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leave-one-out cross-validation NOT USED\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "loo = LeaveOneOut()\n",
    "\n",
    "scores = cross_val_score(logreg, dfall_encoded.loc[:,~dfall_encoded.columns.isin(drop_variables)]\n",
    ",dfall_encoded['target_1'], cv=loo)\n",
    "print(\"Number of cv iterations: \", len(scores))\n",
    "print(\"Mean accuracy: {:.2f}\".format(scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdd9482",
   "metadata": {},
   "source": [
    "### Shuffle-split cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21749cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle-split cross-validation NOT USED\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "shuffle_split = ShuffleSplit(test_size=.5, train_size=.5, n_splits=10)\n",
    "\n",
    "scores = cross_val_score(logreg, dfall_encoded.loc[:,~dfall_encoded.columns.isin(drop_variables)]\n",
    ",dfall_encoded['target_1'], cv=shuffle_split)\n",
    "print(\"Cross-validation scores:\\n{}\".format(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24106958",
   "metadata": {},
   "source": [
    "### Grouped Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e363a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouped Cross-validation NOT USED\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "# create synthetic dataset\n",
    "X, y = make_blobs(n_samples=12, random_state=0)\n",
    "# assume the first three samples belong to the same group,\n",
    "# then the next four, etc.\n",
    "groups = [0, 0, 0, 1, 1, 1, 1, 2, 2, 3, 3, 3]\n",
    "scores = cross_val_score(logreg, X, y, groups, cv=GroupKFold(n_splits=3))\n",
    "print(\"Cross-validation scores:\\n{}\".format(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c3e9eb",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355d886d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search NOT USED\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "# split data into train+validation set and test set\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(iris.data, iris.target, random_state=0)\n",
    "# split train+validation set into training and validation sets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "X_trainval, y_trainval, random_state=1)\n",
    "print(\"Size of training set: {} size of validation set: {} size of test set:\"\n",
    "\" {}\\n\".format(X_train.shape[0], X_valid.shape[0], X_test.shape[0]))\n",
    "best_score = 0\n",
    "for gamma in [0.001, 0.01, 0.1, 1, 10, 100]:\n",
    "    for C in [0.001, 0.01, 0.1, 1, 10, 100]:\n",
    "        # for each combination of parameters, train an SVC\n",
    "        svm = SVC(gamma=gamma, C=C)\n",
    "        svm.fit(X_train, y_train)\n",
    "        # evaluate the SVC on the test set\n",
    "        score = svm.score(X_valid, y_valid)\n",
    "        # if we got a better score, store the score and parameters\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_parameters = {'C': C, 'gamma': gamma}\n",
    "# rebuild a model on the combined training and validation set,\n",
    "# and evaluate it on the test set\n",
    "svm = SVC(**best_parameters)\n",
    "svm.fit(X_trainval, y_trainval)\n",
    "test_score = svm.score(X_test, y_test)\n",
    "print(\"Best score on validation set: {:.2f}\".format(best_score))\n",
    "print(\"Best parameters: \", best_parameters)\n",
    "print(\"Test set score with best parameters: {:.2f}\".format(test_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b6a83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search with Cross-Validation NOT USED\n",
    "for gamma in [0.001, 0.01, 0.1, 1, 10, 100]:\n",
    "    for C in [0.001, 0.01, 0.1, 1, 10, 100]:\n",
    "        # for each combination of parameters,\n",
    "        # train an SVC\n",
    "        svm = SVC(gamma=gamma, C=C)\n",
    "        # perform cross-validation\n",
    "        scores = cross_val_score(svm, X_trainval, y_trainval, cv=5)\n",
    "        # compute mean cross-validation accuracy\n",
    "        score = np.mean(scores)\n",
    "        # if we got a better score, store the score and parameters\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_parameters = {'C': C, 'gamma': gamma}\n",
    "# rebuild a model on the combined training and validation set\n",
    "svm = SVC(**best_parameters)\n",
    "svm.fit(X_trainval, y_trainval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3a41b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=0)\n",
    "\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100],'gamma': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "print(\"Parameter grid:\\n{}\".format(param_grid))\n",
    "\n",
    "grid_search = GridSearchCV(SVC(), param_grid, cv=5, scoring = 'accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Test set score: {:.2f}\".format(grid_search.score(X_test, y_test)))\n",
    "\n",
    "print(\"Best parameters: {}\".format(grid_search.best_params_))\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))\n",
    "print(\"Best estimator:\\n{}\".format(grid_search.best_estimator_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4310722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyzing the result of GridSearvhCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf1909c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid combinations\n",
    "# cv combinations\n",
    "# Parrellization with n_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd10f88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imbalanced Class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a43b925",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f7d2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline in book"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30abc4df",
   "metadata": {},
   "source": [
    "# Rough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4731439a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trees - elbow chart b/w tree size and accuracy, pruning   \n",
    "# Imputation\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
